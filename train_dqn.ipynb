{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import (BATCH_SIZE, CLIP_REWARD, DISCOUNT_FACTOR, ENV_NAME,\n",
    "                    EVAL_LENGTH, FRAMES_BETWEEN_EVAL, INPUT_SHAPE,\n",
    "                    LEARNING_RATE, LOAD_FROM, LOAD_REPLAY_BUFFER,\n",
    "                    MAX_EPISODE_LENGTH, MAX_NOOP_STEPS, MEM_SIZE,\n",
    "                    MIN_REPLAY_BUFFER_SIZE, PRIORITY_SCALE, SAVE_PATH,\n",
    "                    TENSORBOARD_DIR, TOTAL_FRAMES, UPDATE_FREQ, USE_PER,\n",
    "                    WRITE_TENSORBOARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.layers import (Add, Conv2D, Dense, Flatten, Input,\n",
    "                                     Lambda, Subtract)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Funktion definieren für resize and greyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can resize to any shape, but was built to resize to 84x84\n",
    "def process_frame(frame, shape=(84, 84)):\n",
    "    \"\"\"Preprocesses a 210x160x3 frame to 84x84x1 grayscale\n",
    "    Arguments:\n",
    "        frame: The frame to process.  Must have values ranging from 0-255\n",
    "    Returns:\n",
    "        The processed frame\n",
    "    \"\"\"\n",
    "    frame = frame.astype(np.uint8) \n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    frame = frame[34:34+160, :160]  # crop image\n",
    "    frame = cv2.resize(frame, shape, interpolation=cv2.INTER_NEAREST)\n",
    "    frame = frame.reshape((*shape, 1))\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Funktion definieren für den Build des Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_network(n_actions, learning_rate=0.00001, input_shape=(84, 84), history_length=4):\n",
    "    \"\"\"Builds a dueling DQN as a Keras model\n",
    "    Arguments:\n",
    "        n_actions: Number of possible action the agent can take\n",
    "        learning_rate: Learning rate\n",
    "        input_shape: Shape of the preprocessed frame the model sees\n",
    "        history_length: Number of historical frames the agent can see\n",
    "    Returns:\n",
    "        A compiled Keras model\n",
    "    \"\"\"\n",
    "    model_input = Input(shape=(input_shape[0], input_shape[1], history_length))\n",
    "    x = Lambda(lambda layer: layer / 255)(model_input)  # normalize by 255\n",
    "\n",
    "    x = Conv2D(32, (8, 8), strides=4, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(64, (4, 4), strides=2, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(64, (3, 3), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "    x = Conv2D(1024, (7, 7), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)(x)\n",
    "\n",
    "    # Split into value and advantage streams\n",
    "    val_stream, adv_stream = Lambda(lambda w: tf.split(w, 2, 3))(x)  # custom splitting layer\n",
    "\n",
    "    val_stream = Flatten()(val_stream)\n",
    "    val = Dense(1, kernel_initializer=VarianceScaling(scale=2.))(val_stream)\n",
    "\n",
    "    adv_stream = Flatten()(adv_stream)\n",
    "    adv = Dense(n_actions, kernel_initializer=VarianceScaling(scale=2.))(adv_stream)\n",
    "\n",
    "    # Combine streams into Q-Values\n",
    "    reduce_mean = Lambda(lambda w: tf.reduce_mean(w, axis=1, keepdims=True))  # custom layer for reduce mean\n",
    "\n",
    "    q_vals = Add()([val, Subtract()([adv, reduce_mean(adv)])])\n",
    "\n",
    "    # Build model\n",
    "    model = Model(model_input, q_vals)\n",
    "    model.compile(Adam(learning_rate), loss=tf.keras.losses.Huber())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Funktion zum aufsetzen von Gym Umfeld von OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameWrapper:\n",
    "    \"\"\"Wrapper for the environment provided by Gym\"\"\"\n",
    "    def __init__(self, env_name, no_op_steps=10, history_length=4):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.history_length = 4\n",
    "\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "\n",
    "    def reset(self, evaluation=False):\n",
    "        \"\"\"Resets the environment\n",
    "        Arguments:\n",
    "            evaluation: Set to True when the agent is being evaluated. Takes a random number of no-op steps if True.\n",
    "        \"\"\"\n",
    "\n",
    "        self.frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "\n",
    "        # If evaluating, take a random number of no-op steps.\n",
    "        # This adds an element of randomness, so that the each\n",
    "        # evaluation is slightly different.\n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(0, self.no_op_steps)):\n",
    "                self.env.step(1)\n",
    "\n",
    "        # For the initial state, we stack the first frame four times\n",
    "        self.state = np.repeat(process_frame(self.frame), self.history_length, axis=2)\n",
    "\n",
    "    def step(self, action, render_mode=None):\n",
    "        \"\"\"Performs an action and observes the result\n",
    "        Arguments:\n",
    "            action: An integer describe action the agent chose\n",
    "            render_mode: None doesn't render anything, 'human' renders the screen in a new window, 'rgb_array' returns an np.array with rgb values\n",
    "        Returns:\n",
    "            processed_frame: The processed new frame as a result of that action\n",
    "            reward: The reward for taking that action\n",
    "            terminal: Whether the game has ended\n",
    "            life_lost: Whether a life has been lost\n",
    "            new_frame: The raw new frame as a result of that action\n",
    "            If render_mode is set to 'rgb_array' this also returns the rendered rgb_array\n",
    "        \"\"\"\n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "\n",
    "        # In the commonly ignored 'info' or 'meta' data returned by env.step\n",
    "        # we can get information such as the number of lives the agent has.\n",
    "\n",
    "        # We use this here to find out when the agent loses a life, and\n",
    "        # if so, we set life_lost to True.\n",
    "\n",
    "        # We use life_lost to force the agent to start the game\n",
    "        # and not sit around doing nothing.\n",
    "        if info['ale.lives'] < self.last_lives:\n",
    "            life_lost = True\n",
    "        else:\n",
    "            life_lost = terminal\n",
    "        self.last_lives = info['ale.lives']\n",
    "\n",
    "        processed_frame = process_frame(new_frame)\n",
    "        self.state = np.append(self.state[:, :, 1:], processed_frame, axis=2)\n",
    "\n",
    "        if render_mode == 'rgb_array':\n",
    "            return processed_frame, reward, terminal, life_lost, self.env.render(render_mode)\n",
    "        elif render_mode == 'human':\n",
    "            self.env.render()\n",
    "\n",
    "        return processed_frame, reward, terminal, life_lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Klasse definieren damit der Replay Buffer genutzt werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay Buffer to store transitions.\n",
    "    This implementation was heavily inspired by Fabio M. Graetz's replay buffer\n",
    "    here: https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\"\"\"\n",
    "    def __init__(self, size=1000000, input_shape=(84, 84), history_length=4, use_per=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            size: Integer, Number of stored transitions\n",
    "            input_shape: Shape of the preprocessed frame\n",
    "            history_length: Integer, Number of frames stacked together to create a state for the agent\n",
    "            use_per: Use PER instead of classic experience replay\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.input_shape = input_shape\n",
    "        self.history_length = history_length\n",
    "        self.count = 0  # total index of memory written to, always less than self.size\n",
    "        self.current = 0  # index to write to\n",
    "\n",
    "        # Pre-allocate memory\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, self.input_shape[0], self.input_shape[1]), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "        self.priorities = np.zeros(self.size, dtype=np.float32)\n",
    "\n",
    "        self.use_per = use_per\n",
    "\n",
    "    def add_experience(self, action, frame, reward, terminal, clip_reward=True):\n",
    "        \"\"\"Saves a transition to the replay buffer\n",
    "        Arguments:\n",
    "            action: An integer between 0 and env.action_space.n - 1 \n",
    "                determining the action the agent perfomed\n",
    "            frame: A (84, 84, 1) frame of the game in grayscale\n",
    "            reward: A float determining the reward the agend received for performing an action\n",
    "            terminal: A bool stating whether the episode terminated\n",
    "        \"\"\"\n",
    "        if frame.shape != self.input_shape:\n",
    "            raise ValueError('Dimension of frame is wrong!')\n",
    "\n",
    "        if clip_reward:\n",
    "            reward = np.sign(reward)\n",
    "\n",
    "        # Write memory\n",
    "        self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.priorities[self.current] = max(self.priorities.max(), 1)  # make the most recent experience important\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "\n",
    "    def get_minibatch(self, batch_size=32, priority_scale=0.0):\n",
    "        \"\"\"Returns a minibatch of self.batch_size = 32 transitions\n",
    "        Arguments:\n",
    "            batch_size: How many samples to return\n",
    "            priority_scale: How much to weight priorities. 0 = completely random, 1 = completely based on priority\n",
    "        Returns:\n",
    "            A tuple of states, actions, rewards, new_states, and terminals\n",
    "            If use_per is True:\n",
    "                An array describing the importance of transition. Used for scaling gradient steps.\n",
    "                An array of each index that was sampled\n",
    "        \"\"\"\n",
    "\n",
    "        if self.count < self.history_length:\n",
    "            raise ValueError('Not enough memories to get a minibatch')\n",
    "\n",
    "        # Get sampling probabilities from priority list\n",
    "        if self.use_per:\n",
    "            scaled_priorities = self.priorities[self.history_length:self.count-1] ** priority_scale\n",
    "            sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "\n",
    "        # Get a list of valid indices\n",
    "        indices = []\n",
    "        for i in range(batch_size):\n",
    "            while True:\n",
    "                # Get a random number from history_length to maximum frame written with probabilities based on priority weights\n",
    "                if self.use_per:\n",
    "                    index = np.random.choice(np.arange(self.history_length, self.count-1), p=sample_probabilities)\n",
    "                else:\n",
    "                    index = random.randint(self.history_length, self.count - 1)\n",
    "\n",
    "                # We check that all frames are from same episode with the two following if statements.  If either are True, the index is invalid.\n",
    "                if index >= self.current and index - self.history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            indices.append(index)\n",
    "\n",
    "        # Retrieve states from memory\n",
    "        states = []\n",
    "        new_states = []\n",
    "        for idx in indices:\n",
    "            states.append(self.frames[idx-self.history_length:idx, ...])\n",
    "            new_states.append(self.frames[idx-self.history_length+1:idx+1, ...])\n",
    "\n",
    "        states = np.transpose(np.asarray(states), axes=(0, 2, 3, 1))\n",
    "        new_states = np.transpose(np.asarray(new_states), axes=(0, 2, 3, 1))\n",
    "\n",
    "        if self.use_per:\n",
    "            # Get importance weights from probabilities calculated earlier\n",
    "            importance = 1/self.count * 1/sample_probabilities[[index - self.history_length for index in indices]]\n",
    "            importance = importance / importance.max()\n",
    "\n",
    "            return (states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]), importance, indices\n",
    "        else:\n",
    "            return states, self.actions[indices], self.rewards[indices], new_states, self.terminal_flags[indices]\n",
    "\n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        \"\"\"Update priorities for PER\n",
    "        Arguments:\n",
    "            indices: Indices to update\n",
    "            errors: For each index, the error between the target Q-vals and the predicted Q-vals\n",
    "        \"\"\"\n",
    "        for i, e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + offset\n",
    "\n",
    "    def save(self, folder_name):\n",
    "        \"\"\"Save the replay buffer to a folder\"\"\"\n",
    "\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.mkdir(folder_name)\n",
    "\n",
    "        np.save(folder_name + '/actions.npy', self.actions)\n",
    "        np.save(folder_name + '/frames.npy', self.frames)\n",
    "        np.save(folder_name + '/rewards.npy', self.rewards)\n",
    "        np.save(folder_name + '/terminal_flags.npy', self.terminal_flags)\n",
    "\n",
    "    def load(self, folder_name):\n",
    "        \"\"\"Loads the replay buffer from a folder\"\"\"\n",
    "        self.actions = np.load(folder_name + '/actions.npy')\n",
    "        self.frames = np.load(folder_name + '/frames.npy')\n",
    "        self.rewards = np.load(folder_name + '/rewards.npy')\n",
    "        self.terminal_flags = np.load(folder_name + '/terminal_flags.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Klasse des Agenten definieren damit dieser mit seinen Funktionen genutzt werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"Implements a standard DDDQN agent\"\"\"\n",
    "    def __init__(self,\n",
    "                 dqn,\n",
    "                 target_dqn,\n",
    "                 replay_buffer,\n",
    "                 n_actions,\n",
    "                 input_shape=(84, 84),\n",
    "                 batch_size=32,\n",
    "                 history_length=4,\n",
    "                 eps_initial=1,\n",
    "                 eps_final=0.1,\n",
    "                 eps_final_frame=0.01,\n",
    "                 eps_evaluation=0.0,\n",
    "                 eps_annealing_frames=1000000,\n",
    "                 replay_buffer_start_size=50000,\n",
    "                 max_frames=25000000,\n",
    "                 use_per=True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            dqn: A DQN (returned by the DQN function) to predict moves\n",
    "            target_dqn: A DQN (returned by the DQN function) to predict target-q values.  This can be initialized in the same way as the dqn argument\n",
    "            replay_buffer: A ReplayBuffer object for holding all previous experiences\n",
    "            n_actions: Number of possible actions for the given environment\n",
    "            input_shape: Tuple/list describing the shape of the pre-processed environment\n",
    "            batch_size: Number of samples to draw from the replay memory every updating session\n",
    "            history_length: Number of historical frames available to the agent\n",
    "            eps_initial: Initial epsilon value.\n",
    "            eps_final: The \"half-way\" epsilon value.  The epsilon value decreases more slowly after this\n",
    "            eps_final_frame: The final epsilon value\n",
    "            eps_evaluation: The epsilon value used during evaluation\n",
    "            eps_annealing_frames: Number of frames during which epsilon will be annealed to eps_final, then eps_final_frame\n",
    "            replay_buffer_start_size: Size of replay buffer before beginning to learn (after this many frames, epsilon is decreased more slowly)\n",
    "            max_frames: Number of total frames the agent will be trained for\n",
    "            use_per: Use PER instead of classic experience replay\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.input_shape = input_shape\n",
    "        self.history_length = history_length\n",
    "\n",
    "        # Memory information\n",
    "        self.replay_buffer_start_size = replay_buffer_start_size\n",
    "        self.max_frames = max_frames\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.use_per = use_per\n",
    "\n",
    "        # Epsilon information\n",
    "        self.eps_initial = eps_initial\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_final_frame = eps_final_frame\n",
    "        self.eps_evaluation = eps_evaluation\n",
    "        self.eps_annealing_frames = eps_annealing_frames\n",
    "\n",
    "        # Slopes and intercepts for exploration decrease\n",
    "        # (Credit to Fabio M. Graetz for this and calculating epsilon based on frame number)\n",
    "        self.slope = -(self.eps_initial - self.eps_final) / self.eps_annealing_frames\n",
    "        self.intercept = self.eps_initial - self.slope*self.replay_buffer_start_size\n",
    "        self.slope_2 = -(self.eps_final - self.eps_final_frame) / (self.max_frames - self.eps_annealing_frames - self.replay_buffer_start_size)\n",
    "        self.intercept_2 = self.eps_final_frame - self.slope_2*self.max_frames\n",
    "\n",
    "        # DQN\n",
    "        self.DQN = dqn\n",
    "        self.target_dqn = target_dqn\n",
    "\n",
    "    def calc_epsilon(self, frame_number, evaluation=False):\n",
    "        \"\"\"Get the appropriate epsilon value from a given frame number\n",
    "        Arguments:\n",
    "            frame_number: Global frame number (used for epsilon)\n",
    "            evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
    "        Returns:\n",
    "            The appropriate epsilon value\n",
    "        \"\"\"\n",
    "        if evaluation:\n",
    "            return self.eps_evaluation\n",
    "        elif frame_number < self.replay_buffer_start_size:\n",
    "            return self.eps_initial\n",
    "        elif frame_number >= self.replay_buffer_start_size and frame_number < self.replay_buffer_start_size + self.eps_annealing_frames:\n",
    "            return self.slope*frame_number + self.intercept\n",
    "        elif frame_number >= self.replay_buffer_start_size + self.eps_annealing_frames:\n",
    "            return self.slope_2*frame_number + self.intercept_2\n",
    "\n",
    "    def get_action(self, frame_number, state, evaluation=False):\n",
    "        \"\"\"Query the DQN for an action given a state\n",
    "        Arguments:\n",
    "            frame_number: Global frame number (used for epsilon)\n",
    "            state: State to give an action for\n",
    "            evaluation: True if the model is evaluating, False otherwise (uses eps_evaluation instead of default epsilon value)\n",
    "        Returns:\n",
    "            An integer as the predicted move\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate epsilon based on the frame number\n",
    "        eps = self.calc_epsilon(frame_number, evaluation)\n",
    "\n",
    "        # With chance epsilon, take a random action\n",
    "        if np.random.rand(1) < eps:\n",
    "            return np.random.randint(0, self.n_actions)\n",
    "\n",
    "        # Otherwise, query the DQN for an action\n",
    "        q_vals = self.DQN.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))[0]\n",
    "        return q_vals.argmax()\n",
    "\n",
    "    def get_intermediate_representation(self, state, layer_names=None, stack_state=True):\n",
    "        \"\"\"\n",
    "        Get the output of a hidden layer inside the model.  This will be/is used for visualizing model\n",
    "        Arguments:\n",
    "            state: The input to the model to get outputs for hidden layers from\n",
    "            layer_names: Names of the layers to get outputs from.  This can be a list of multiple names, or a single name\n",
    "            stack_state: Stack `state` four times so the model can take input on a single (84, 84, 1) frame\n",
    "        Returns:\n",
    "            Outputs to the hidden layers specified, in the order they were specified.\n",
    "        \"\"\"\n",
    "        # Prepare list of layers\n",
    "        if isinstance(layer_names, list) or isinstance(layer_names, tuple):\n",
    "            layers = [self.DQN.get_layer(name=layer_name).output for layer_name in layer_names]\n",
    "        else:\n",
    "            layers = self.DQN.get_layer(name=layer_names).output\n",
    "\n",
    "        # Model for getting intermediate output\n",
    "        temp_model = tf.keras.Model(self.DQN.inputs, layers)\n",
    "\n",
    "        # Stack state 4 times\n",
    "        if stack_state:\n",
    "            if len(state.shape) == 2:\n",
    "                state = state[:, :, np.newaxis]\n",
    "            state = np.repeat(state, self.history_length, axis=2)\n",
    "\n",
    "        # Put it all together\n",
    "        return temp_model.predict(state.reshape((-1, self.input_shape[0], self.input_shape[1], self.history_length)))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update the target Q network\"\"\"\n",
    "        self.target_dqn.set_weights(self.DQN.get_weights())\n",
    "\n",
    "    def add_experience(self, action, frame, reward, terminal, clip_reward=True):\n",
    "        \"\"\"Wrapper function for adding an experience to the Agent's replay buffer\"\"\"\n",
    "        self.replay_buffer.add_experience(action, frame, reward, terminal, clip_reward)\n",
    "\n",
    "    def learn(self, batch_size, gamma, frame_number, priority_scale=1.0):\n",
    "        \"\"\"Sample a batch and use it to improve the DQN\n",
    "        Arguments:\n",
    "            batch_size: How many samples to draw for an update\n",
    "            gamma: Reward discount\n",
    "            frame_number: Global frame number (used for calculating importances)\n",
    "            priority_scale: How much to weight priorities when sampling the replay buffer. 0 = completely random, 1 = completely based on priority\n",
    "        Returns:\n",
    "            The loss between the predicted and target Q as a float\n",
    "        \"\"\"\n",
    "\n",
    "        if self.use_per:\n",
    "            (states, actions, rewards, new_states, terminal_flags), importance, indices = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
    "            importance = importance ** (1-self.calc_epsilon(frame_number))\n",
    "        else:\n",
    "            states, actions, rewards, new_states, terminal_flags = self.replay_buffer.get_minibatch(batch_size=self.batch_size, priority_scale=priority_scale)\n",
    "\n",
    "        # Main DQN estimates best action in new states\n",
    "        arg_q_max = self.DQN.predict(new_states).argmax(axis=1)\n",
    "\n",
    "        # Target DQN estimates q-vals for new states\n",
    "        future_q_vals = self.target_dqn.predict(new_states)\n",
    "        double_q = future_q_vals[range(batch_size), arg_q_max]\n",
    "\n",
    "        # Calculate targets (bellman equation)\n",
    "        target_q = rewards + (gamma*double_q * (1-terminal_flags))\n",
    "\n",
    "        # Use targets to calculate loss (and use loss to calculate gradients)\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.DQN(states)\n",
    "\n",
    "            one_hot_actions = tf.keras.utils.to_categorical(actions, self.n_actions, dtype=np.float32)  # using tf.one_hot causes strange errors\n",
    "            Q = tf.reduce_sum(tf.multiply(q_values, one_hot_actions), axis=1)\n",
    "\n",
    "            error = Q - target_q\n",
    "            loss = tf.keras.losses.Huber()(target_q, Q)\n",
    "\n",
    "            if self.use_per:\n",
    "                # Multiply the loss by importance, so that the gradient is also scaled.\n",
    "                # The importance scale reduces bias against situataions that are sampled\n",
    "                # more frequently.\n",
    "                loss = tf.reduce_mean(loss * importance)\n",
    "\n",
    "        model_gradients = tape.gradient(loss, self.DQN.trainable_variables)\n",
    "        self.DQN.optimizer.apply_gradients(zip(model_gradients, self.DQN.trainable_variables))\n",
    "\n",
    "        if self.use_per:\n",
    "            self.replay_buffer.set_priorities(indices, error)\n",
    "\n",
    "        return float(loss.numpy()), error\n",
    "\n",
    "    def save(self, folder_name, **kwargs):\n",
    "        \"\"\"Saves the Agent and all corresponding properties into a folder\n",
    "        Arguments:\n",
    "            folder_name: Folder in which to save the Agent\n",
    "            **kwargs: Agent.save will also save any keyword arguments passed.  This is used for saving the frame_number\n",
    "        \"\"\"\n",
    "\n",
    "        # Create the folder for saving the agent\n",
    "        if not os.path.isdir(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        # Save DQN and target DQN\n",
    "        self.DQN.save(folder_name + '/dqn.h5')\n",
    "        self.target_dqn.save(folder_name + '/target_dqn.h5')\n",
    "\n",
    "        # Save replay buffer\n",
    "        self.replay_buffer.save(folder_name + '/replay-buffer')\n",
    "\n",
    "        # Save meta\n",
    "        with open(folder_name + '/meta.json', 'w+') as f:\n",
    "            f.write(json.dumps({**{'buff_count': self.replay_buffer.count, 'buff_curr': self.replay_buffer.current}, **kwargs}))  # save replay_buffer information and any other information\n",
    "\n",
    "    def load(self, folder_name, load_replay_buffer=True):\n",
    "        \"\"\"Load a previously saved Agent from a folder\n",
    "        Arguments:\n",
    "            folder_name: Folder from which to load the Agent\n",
    "        Returns:\n",
    "            All other saved attributes, e.g., frame number\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.isdir(folder_name):\n",
    "            raise ValueError(f'{folder_name} is not a valid directory')\n",
    "\n",
    "        # Load DQNs\n",
    "        self.DQN = tf.keras.models.load_model(folder_name + '/dqn.h5')\n",
    "        self.target_dqn = tf.keras.models.load_model(folder_name + '/target_dqn.h5')\n",
    "        self.optimizer = self.DQN.optimizer\n",
    "\n",
    "        # Load replay buffer\n",
    "        if load_replay_buffer:\n",
    "            self.replay_buffer.load(folder_name + '/replay-buffer')\n",
    "\n",
    "        # Load meta\n",
    "        with open(folder_name + '/meta.json', 'r') as f:\n",
    "            meta = json.load(f)\n",
    "\n",
    "        if load_replay_buffer:\n",
    "            self.replay_buffer.count = meta['buff_count']\n",
    "            self.replay_buffer.current = meta['buff_curr']\n",
    "\n",
    "        del meta['buff_count'], meta['buff_curr']  # we don't want to return this information\n",
    "        return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Umgebung aufsetzen/ Replay Buffer initalisieren/ Agent initalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has the following 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DQN-env/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "game_wrapper = GameWrapper(ENV_NAME, MAX_NOOP_STEPS)\n",
    "print(\"The environment has the following {} actions: {}\".format(game_wrapper.env.action_space.n, game_wrapper.env.unwrapped.get_action_meanings()))\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = tf.summary.create_file_writer(TENSORBOARD_DIR)\n",
    "\n",
    "# Build main and target networks\n",
    "MAIN_DQN = build_q_network(game_wrapper.env.action_space.n, LEARNING_RATE, input_shape=INPUT_SHAPE)\n",
    "TARGET_DQN = build_q_network(game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE)\n",
    "\n",
    "replay_buffer = ReplayBuffer(size=MEM_SIZE, input_shape=INPUT_SHAPE, use_per=USE_PER)\n",
    "agent = Agent(MAIN_DQN, TARGET_DQN, replay_buffer, game_wrapper.env.action_space.n, input_shape=INPUT_SHAPE, batch_size=BATCH_SIZE, use_per=USE_PER)\n",
    "\n",
    "# Training and evaluation\n",
    "if LOAD_FROM is None:\n",
    "    frame_number = 0\n",
    "    rewards = []\n",
    "    loss_list = []\n",
    "else:\n",
    "    print('Loading from', LOAD_FROM)\n",
    "    meta = agent.load(LOAD_FROM, LOAD_REPLAY_BUFFER)\n",
    "\n",
    "    # Apply information loaded from meta\n",
    "    frame_number = meta['frame_number']\n",
    "    rewards = meta['rewards']\n",
    "    loss_list = meta['loss_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Trainings Loop starten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DQN-env/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/envs/DQN-env/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game number: 000010  Frame number: 00001878  Average reward: 1.3  Time taken: 0.4s\n",
      "Game number: 000020  Frame number: 00003616  Average reward: 1.0  Time taken: 0.2s\n",
      "Game number: 000030  Frame number: 00005255  Average reward: 0.7  Time taken: 0.3s\n",
      "Game number: 000040  Frame number: 00006794  Average reward: 0.4  Time taken: 0.2s\n",
      "Game number: 000050  Frame number: 00008758  Average reward: 1.6  Time taken: 0.3s\n",
      "Game number: 000060  Frame number: 00010271  Average reward: 0.5  Time taken: 0.2s\n",
      "Game number: 000070  Frame number: 00011896  Average reward: 0.7  Time taken: 0.2s\n",
      "Game number: 000080  Frame number: 00013535  Average reward: 0.7  Time taken: 0.2s\n",
      "Game number: 000090  Frame number: 00015078  Average reward: 0.4  Time taken: 0.2s\n",
      "Game number: 000100  Frame number: 00016736  Average reward: 0.7  Time taken: 0.2s\n",
      "Game number: 000110  Frame number: 00018483  Average reward: 1.2  Time taken: 0.2s\n",
      "Game number: 000120  Frame number: 00020726  Average reward: 2.2  Time taken: 0.4s\n",
      "Game number: 000130  Frame number: 00022522  Average reward: 1.1  Time taken: 0.2s\n",
      "Game number: 000140  Frame number: 00024206  Average reward: 0.9  Time taken: 0.3s\n",
      "Game number: 000150  Frame number: 00026116  Average reward: 1.4  Time taken: 0.2s\n",
      "Game number: 000160  Frame number: 00027854  Average reward: 1.1  Time taken: 0.3s\n",
      "Game number: 000170  Frame number: 00029497  Average reward: 0.8  Time taken: 0.2s\n",
      "Game number: 000180  Frame number: 00031399  Average reward: 1.4  Time taken: 0.2s\n",
      "Game number: 000190  Frame number: 00033200  Average reward: 1.2  Time taken: 0.3s\n",
      "Game number: 000200  Frame number: 00034897  Average reward: 0.9  Time taken: 0.2s\n",
      "Game number: 000210  Frame number: 00036841  Average reward: 1.8  Time taken: 0.2s\n",
      "Game number: 000220  Frame number: 00038528  Average reward: 0.8  Time taken: 0.2s\n",
      "Game number: 000230  Frame number: 00040131  Average reward: 0.8  Time taken: 0.2s\n",
      "Game number: 000240  Frame number: 00042013  Average reward: 1.5  Time taken: 0.2s\n",
      "Game number: 000250  Frame number: 00043731  Average reward: 0.9  Time taken: 0.2s\n",
      "Game number: 000260  Frame number: 00045759  Average reward: 1.7  Time taken: 0.3s\n",
      "Game number: 000270  Frame number: 00047702  Average reward: 1.5  Time taken: 0.2s\n",
      "Game number: 000280  Frame number: 00049285  Average reward: 0.7  Time taken: 0.2s\n",
      "Game number: 000290  Frame number: 00051107  Average reward: 1.1  Time taken: 16.3s\n",
      "Game number: 000300  Frame number: 00052821  Average reward: 1.0  Time taken: 14.6s\n",
      "Game number: 000310  Frame number: 00054609  Average reward: 1.1  Time taken: 12.2s\n",
      "Game number: 000320  Frame number: 00056478  Average reward: 1.2  Time taken: 18.8s\n",
      "Game number: 000330  Frame number: 00058252  Average reward: 1.2  Time taken: 13.8s\n",
      "Game number: 000340  Frame number: 00059737  Average reward: 0.4  Time taken: 11.1s\n",
      "Game number: 000350  Frame number: 00061658  Average reward: 1.5  Time taken: 10.8s\n",
      "Game number: 000360  Frame number: 00063704  Average reward: 1.8  Time taken: 19.0s\n",
      "Game number: 000370  Frame number: 00065311  Average reward: 0.7  Time taken: 12.2s\n",
      "Game number: 000380  Frame number: 00066896  Average reward: 0.6  Time taken: 13.4s\n",
      "Game number: 000390  Frame number: 00068755  Average reward: 1.3  Time taken: 11.4s\n",
      "Game number: 000400  Frame number: 00070712  Average reward: 1.5  Time taken: 18.9s\n",
      "Game number: 000410  Frame number: 00072479  Average reward: 1.1  Time taken: 14.7s\n",
      "Game number: 000420  Frame number: 00074119  Average reward: 0.7  Time taken: 12.1s\n",
      "Game number: 000430  Frame number: 00076158  Average reward: 1.8  Time taken: 11.5s\n",
      "Game number: 000440  Frame number: 00077735  Average reward: 0.5  Time taken: 16.6s\n",
      "Game number: 000450  Frame number: 00079741  Average reward: 1.6  Time taken: 10.9s\n",
      "Game number: 000460  Frame number: 00081744  Average reward: 1.5  Time taken: 16.1s\n",
      "Game number: 000470  Frame number: 00083531  Average reward: 1.3  Time taken: 14.7s\n",
      "Game number: 000480  Frame number: 00085138  Average reward: 0.7  Time taken: 13.2s\n",
      "Game number: 000490  Frame number: 00087092  Average reward: 1.7  Time taken: 13.0s\n",
      "Game number: 000500  Frame number: 00088903  Average reward: 1.4  Time taken: 9.1s\n",
      "Game number: 000510  Frame number: 00090703  Average reward: 1.2  Time taken: 12.2s\n",
      "Game number: 000520  Frame number: 00092641  Average reward: 1.6  Time taken: 13.2s\n",
      "Game number: 000530  Frame number: 00094487  Average reward: 1.4  Time taken: 16.2s\n",
      "Game number: 000540  Frame number: 00096530  Average reward: 1.8  Time taken: 17.1s\n",
      "Game number: 000550  Frame number: 00098631  Average reward: 2.2  Time taken: 11.8s\n",
      "Evaluation score: 0.0\n",
      "Game number: 000560  Frame number: 00100406  Average reward: 1.2  Time taken: 14.2s\n",
      "Game number: 000570  Frame number: 00102258  Average reward: 1.4  Time taken: 18.4s\n",
      "Game number: 000580  Frame number: 00104245  Average reward: 1.7  Time taken: 15.8s\n",
      "Game number: 000590  Frame number: 00106135  Average reward: 1.4  Time taken: 19.8s\n",
      "Game number: 000600  Frame number: 00107962  Average reward: 1.4  Time taken: 10.7s\n",
      "Game number: 000610  Frame number: 00109779  Average reward: 1.2  Time taken: 14.2s\n",
      "Game number: 000620  Frame number: 00111799  Average reward: 1.6  Time taken: 11.1s\n",
      "Game number: 000630  Frame number: 00113785  Average reward: 1.8  Time taken: 12.4s\n",
      "Game number: 000640  Frame number: 00115650  Average reward: 1.7  Time taken: 14.1s\n",
      "Game number: 000650  Frame number: 00117893  Average reward: 2.7  Time taken: 16.8s\n",
      "Game number: 000660  Frame number: 00119621  Average reward: 1.2  Time taken: 11.6s\n",
      "Game number: 000670  Frame number: 00121476  Average reward: 1.4  Time taken: 23.5s\n",
      "Game number: 000680  Frame number: 00123411  Average reward: 1.7  Time taken: 10.9s\n",
      "Game number: 000690  Frame number: 00125229  Average reward: 1.4  Time taken: 12.5s\n",
      "Game number: 000700  Frame number: 00127182  Average reward: 1.7  Time taken: 15.5s\n",
      "Game number: 000710  Frame number: 00129301  Average reward: 2.2  Time taken: 14.3s\n",
      "Game number: 000720  Frame number: 00131008  Average reward: 1.1  Time taken: 13.2s\n",
      "Game number: 000730  Frame number: 00132825  Average reward: 1.3  Time taken: 17.9s\n",
      "Game number: 000740  Frame number: 00134571  Average reward: 1.1  Time taken: 14.4s\n",
      "Game number: 000750  Frame number: 00136622  Average reward: 2.2  Time taken: 17.3s\n",
      "Game number: 000760  Frame number: 00138557  Average reward: 1.7  Time taken: 19.7s\n",
      "Game number: 000770  Frame number: 00140778  Average reward: 2.7  Time taken: 10.3s\n",
      "Game number: 000780  Frame number: 00142785  Average reward: 1.7  Time taken: 10.7s\n",
      "Game number: 000790  Frame number: 00144866  Average reward: 2.2  Time taken: 17.0s\n",
      "Game number: 000800  Frame number: 00146976  Average reward: 2.3  Time taken: 14.4s\n",
      "Game number: 000810  Frame number: 00148796  Average reward: 1.5  Time taken: 16.5s\n",
      "Game number: 000820  Frame number: 00150804  Average reward: 2.0  Time taken: 10.5s\n",
      "Game number: 000830  Frame number: 00152678  Average reward: 1.6  Time taken: 18.4s\n",
      "Game number: 000840  Frame number: 00154750  Average reward: 1.7  Time taken: 28.9s\n",
      "Game number: 000850  Frame number: 00156507  Average reward: 1.4  Time taken: 13.7s\n",
      "Game number: 000860  Frame number: 00158384  Average reward: 1.6  Time taken: 20.0s\n",
      "Game number: 000870  Frame number: 00160741  Average reward: 2.7  Time taken: 17.5s\n",
      "Game number: 000880  Frame number: 00162897  Average reward: 2.1  Time taken: 19.5s\n",
      "Game number: 000890  Frame number: 00165159  Average reward: 2.3  Time taken: 15.5s\n",
      "Game number: 000900  Frame number: 00167190  Average reward: 2.1  Time taken: 15.8s\n",
      "Game number: 000910  Frame number: 00169215  Average reward: 1.9  Time taken: 10.0s\n",
      "Game number: 000920  Frame number: 00171035  Average reward: 1.5  Time taken: 18.0s\n",
      "Game number: 000930  Frame number: 00173010  Average reward: 1.8  Time taken: 12.7s\n",
      "Game number: 000940  Frame number: 00174856  Average reward: 1.9  Time taken: 10.4s\n",
      "Game number: 000950  Frame number: 00176934  Average reward: 2.2  Time taken: 11.4s\n",
      "Game number: 000960  Frame number: 00179194  Average reward: 2.5  Time taken: 20.6s\n",
      "Game number: 000970  Frame number: 00181129  Average reward: 1.7  Time taken: 11.2s\n",
      "Game number: 000980  Frame number: 00182968  Average reward: 1.6  Time taken: 14.3s\n",
      "Game number: 000990  Frame number: 00184873  Average reward: 1.7  Time taken: 19.2s\n",
      "Game number: 001000  Frame number: 00186837  Average reward: 1.9  Time taken: 13.5s\n",
      "Game number: 001010  Frame number: 00188872  Average reward: 1.8  Time taken: 22.3s\n",
      "Game number: 001020  Frame number: 00191255  Average reward: 2.8  Time taken: 13.4s\n",
      "Game number: 001030  Frame number: 00193379  Average reward: 2.3  Time taken: 21.7s\n",
      "Game number: 001040  Frame number: 00195505  Average reward: 2.3  Time taken: 11.6s\n",
      "Game number: 001050  Frame number: 00197507  Average reward: 1.9  Time taken: 20.6s\n",
      "Game number: 001060  Frame number: 00199439  Average reward: 1.6  Time taken: 10.7s\n",
      "Evaluation score: 6.255813953488372\n",
      "Game number: 001070  Frame number: 00201083  Average reward: 1.0  Time taken: 11.0s\n",
      "Game number: 001080  Frame number: 00203274  Average reward: 2.3  Time taken: 20.2s\n",
      "Game number: 001090  Frame number: 00205216  Average reward: 1.8  Time taken: 14.4s\n",
      "Game number: 001100  Frame number: 00208033  Average reward: 4.1  Time taken: 19.1s\n",
      "Game number: 001110  Frame number: 00209888  Average reward: 1.6  Time taken: 12.5s\n",
      "Game number: 001120  Frame number: 00211932  Average reward: 2.0  Time taken: 14.0s\n",
      "Game number: 001130  Frame number: 00213834  Average reward: 1.7  Time taken: 16.1s\n",
      "Game number: 001140  Frame number: 00215923  Average reward: 2.1  Time taken: 10.6s\n",
      "Game number: 001150  Frame number: 00218393  Average reward: 3.3  Time taken: 19.3s\n",
      "Game number: 001160  Frame number: 00220343  Average reward: 1.9  Time taken: 15.8s\n",
      "Game number: 001170  Frame number: 00222404  Average reward: 2.1  Time taken: 18.5s\n",
      "Game number: 001180  Frame number: 00224222  Average reward: 1.5  Time taken: 20.7s\n",
      "Game number: 001190  Frame number: 00226182  Average reward: 2.0  Time taken: 14.0s\n",
      "Game number: 001200  Frame number: 00228690  Average reward: 3.9  Time taken: 22.7s\n",
      "Game number: 001210  Frame number: 00230627  Average reward: 1.9  Time taken: 17.5s\n",
      "Game number: 001220  Frame number: 00232677  Average reward: 2.1  Time taken: 20.1s\n",
      "Game number: 001230  Frame number: 00234952  Average reward: 2.9  Time taken: 14.2s\n",
      "Game number: 001240  Frame number: 00236777  Average reward: 1.4  Time taken: 14.6s\n",
      "Game number: 001250  Frame number: 00238702  Average reward: 2.2  Time taken: 15.5s\n",
      "Game number: 001260  Frame number: 00240829  Average reward: 2.4  Time taken: 12.1s\n",
      "Game number: 001270  Frame number: 00242795  Average reward: 2.0  Time taken: 17.5s\n",
      "Game number: 001280  Frame number: 00244863  Average reward: 2.2  Time taken: 10.7s\n",
      "Game number: 001290  Frame number: 00246869  Average reward: 2.3  Time taken: 9.8s\n",
      "Game number: 001300  Frame number: 00248935  Average reward: 2.1  Time taken: 13.3s\n",
      "Game number: 001310  Frame number: 00250952  Average reward: 2.4  Time taken: 12.7s\n",
      "Game number: 001320  Frame number: 00253154  Average reward: 2.5  Time taken: 10.1s\n",
      "Game number: 001330  Frame number: 00255024  Average reward: 1.6  Time taken: 14.7s\n",
      "Game number: 001340  Frame number: 00257072  Average reward: 2.3  Time taken: 15.8s\n",
      "Game number: 001350  Frame number: 00259192  Average reward: 2.3  Time taken: 20.8s\n",
      "Game number: 001360  Frame number: 00261342  Average reward: 2.5  Time taken: 15.3s\n",
      "Game number: 001370  Frame number: 00263174  Average reward: 1.6  Time taken: 15.4s\n",
      "Game number: 001380  Frame number: 00265318  Average reward: 2.5  Time taken: 19.4s\n",
      "Game number: 001390  Frame number: 00267626  Average reward: 3.4  Time taken: 35.1s\n",
      "Game number: 001400  Frame number: 00269800  Average reward: 2.6  Time taken: 11.3s\n",
      "Game number: 001410  Frame number: 00271843  Average reward: 2.2  Time taken: 16.9s\n",
      "Game number: 001420  Frame number: 00274037  Average reward: 2.4  Time taken: 12.4s\n",
      "Game number: 001430  Frame number: 00276592  Average reward: 4.0  Time taken: 16.5s\n",
      "Game number: 001440  Frame number: 00278447  Average reward: 1.6  Time taken: 10.1s\n",
      "Game number: 001450  Frame number: 00280877  Average reward: 3.1  Time taken: 16.1s\n",
      "Game number: 001460  Frame number: 00282805  Average reward: 1.6  Time taken: 19.4s\n",
      "Game number: 001470  Frame number: 00285337  Average reward: 3.3  Time taken: 21.6s\n",
      "Game number: 001480  Frame number: 00287440  Average reward: 2.6  Time taken: 18.6s\n",
      "Game number: 001490  Frame number: 00289291  Average reward: 1.8  Time taken: 18.9s\n",
      "Game number: 001500  Frame number: 00291622  Average reward: 2.8  Time taken: 13.8s\n",
      "Game number: 001510  Frame number: 00293544  Average reward: 1.8  Time taken: 11.7s\n",
      "Game number: 001520  Frame number: 00296036  Average reward: 3.3  Time taken: 23.9s\n",
      "Game number: 001530  Frame number: 00297983  Average reward: 2.0  Time taken: 19.9s\n",
      "Game number: 001540  Frame number: 00300137  Average reward: 2.9  Time taken: 11.2s\n",
      "Evaluation score: 3.372093023255814\n",
      "Game number: 001550  Frame number: 00302675  Average reward: 3.4  Time taken: 34.8s\n",
      "Game number: 001560  Frame number: 00304979  Average reward: 3.1  Time taken: 26.5s\n",
      "Game number: 001570  Frame number: 00307263  Average reward: 2.6  Time taken: 36.4s\n",
      "Game number: 001580  Frame number: 00309718  Average reward: 3.5  Time taken: 19.6s\n",
      "Game number: 001590  Frame number: 00311826  Average reward: 2.5  Time taken: 25.2s\n",
      "Game number: 001600  Frame number: 00314252  Average reward: 3.3  Time taken: 23.6s\n",
      "Game number: 001610  Frame number: 00316338  Average reward: 2.4  Time taken: 18.2s\n",
      "Game number: 001620  Frame number: 00318552  Average reward: 2.7  Time taken: 16.6s\n",
      "Game number: 001630  Frame number: 00320774  Average reward: 2.7  Time taken: 20.1s\n",
      "Game number: 001640  Frame number: 00322754  Average reward: 1.9  Time taken: 22.6s\n",
      "Game number: 001650  Frame number: 00325334  Average reward: 3.5  Time taken: 15.5s\n",
      "Game number: 001660  Frame number: 00327905  Average reward: 3.4  Time taken: 664.3s\n",
      "\n",
      "Training exited early.\n",
      "Saving...\n",
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "try:\n",
    "    with writer.as_default():\n",
    "        while frame_number < TOTAL_FRAMES:\n",
    "            # Training\n",
    "\n",
    "            epoch_frame = 0\n",
    "            while epoch_frame < FRAMES_BETWEEN_EVAL:\n",
    "                start_time = time.time()\n",
    "                game_wrapper.reset()\n",
    "                life_lost = True\n",
    "                episode_reward_sum = 0\n",
    "                for _ in range(MAX_EPISODE_LENGTH):\n",
    "                    # Get action\n",
    "                    action = agent.get_action(frame_number, game_wrapper.state)\n",
    "\n",
    "                    # Take step\n",
    "                    processed_frame, reward, terminal, life_lost = game_wrapper.step(action)\n",
    "                    frame_number += 1\n",
    "                    epoch_frame += 1\n",
    "                    episode_reward_sum += reward\n",
    "\n",
    "                    # Add experience to replay memory\n",
    "                    agent.add_experience(action=action,\n",
    "                                        frame=processed_frame[:, :, 0],\n",
    "                                        reward=reward, clip_reward=CLIP_REWARD,\n",
    "                                        terminal=life_lost)\n",
    "\n",
    "                    # Update agent\n",
    "                    if frame_number % UPDATE_FREQ == 0 and agent.replay_buffer.count > MIN_REPLAY_BUFFER_SIZE:\n",
    "                        loss, _ = agent.learn(BATCH_SIZE, gamma=DISCOUNT_FACTOR, frame_number=frame_number, priority_scale=PRIORITY_SCALE)\n",
    "                        loss_list.append(loss)\n",
    "\n",
    "                    # Update target network\n",
    "                    if frame_number % UPDATE_FREQ == 0 and frame_number > MIN_REPLAY_BUFFER_SIZE:\n",
    "                        agent.update_target_network()\n",
    "\n",
    "                    # Break the loop when the game is over\n",
    "                    if terminal:\n",
    "                        terminal = False\n",
    "                        break\n",
    "\n",
    "                rewards.append(episode_reward_sum)\n",
    "\n",
    "                # Output the progress every 10 games\n",
    "                if len(rewards) % 10 == 0:\n",
    "                    # Write to TensorBoard\n",
    "                    if WRITE_TENSORBOARD:\n",
    "                        tf.summary.scalar('Reward', np.mean(rewards[-10:]), frame_number)\n",
    "                        tf.summary.scalar('Loss', np.mean(loss_list[-100:]), frame_number)\n",
    "                        writer.flush()\n",
    "\n",
    "                    print(f'Game number: {str(len(rewards)).zfill(6)}  Frame number: {str(frame_number).zfill(8)}  Average reward: {np.mean(rewards[-10:]):0.1f}  Time taken: {(time.time() - start_time):.1f}s')\n",
    "\n",
    "            # Evaluation every `FRAMES_BETWEEN_EVAL` frames\n",
    "            terminal = True\n",
    "            eval_rewards = []\n",
    "            evaluate_frame_number = 0\n",
    "\n",
    "            for _ in range(EVAL_LENGTH):\n",
    "                if terminal:\n",
    "                    game_wrapper.reset(evaluation=True)\n",
    "                    life_lost = True\n",
    "                    episode_reward_sum = 0\n",
    "                    terminal = False\n",
    "\n",
    "                # Breakout requires a \"fire\" action (action #1) to start the\n",
    "                # game each time a life is lost.\n",
    "                # Otherwise, the agent would sit around doing nothing.\n",
    "                action = 1 if life_lost else agent.get_action(frame_number, game_wrapper.state, evaluation=True)\n",
    "\n",
    "                # Step action\n",
    "                _, reward, terminal, life_lost = game_wrapper.step(action)\n",
    "                evaluate_frame_number += 1\n",
    "                episode_reward_sum += reward\n",
    "\n",
    "                # On game-over\n",
    "                if terminal:\n",
    "                    eval_rewards.append(episode_reward_sum)\n",
    "\n",
    "            if len(eval_rewards) > 0:\n",
    "                final_score = np.mean(eval_rewards)\n",
    "            else:\n",
    "                # In case the game is longer than the number of frames allowed\n",
    "                final_score = episode_reward_sum\n",
    "            # Print score and write to tensorboard\n",
    "            print('Evaluation score:', final_score)\n",
    "            if WRITE_TENSORBOARD:\n",
    "                tf.summary.scalar('Evaluation score', final_score, frame_number)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model\n",
    "            if len(rewards) > 300 and SAVE_PATH is not None:\n",
    "                agent.save(f'{SAVE_PATH}/save-{str(frame_number).zfill(8)}', frame_number=frame_number, rewards=rewards, loss_list=loss_list)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nTraining exited early.')\n",
    "    writer.close()\n",
    "\n",
    "    if SAVE_PATH is None:\n",
    "        try:\n",
    "            SAVE_PATH = input('Would you like to save the trained model? If so, type in a save path, otherwise, interrupt with ctrl+c. ')\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nExiting...')\n",
    "\n",
    "    if SAVE_PATH is not None:\n",
    "        print('Saving...')\n",
    "        agent.save(f'{SAVE_PATH}/save-{str(frame_number).zfill(8)}', frame_number=frame_number, rewards=rewards, loss_list=loss_list)\n",
    "        print('Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle:\n",
    "- https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb\n",
    "- https://github.com/sebtheiler/tutorials/tree/main/dqn (Code Quelle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
